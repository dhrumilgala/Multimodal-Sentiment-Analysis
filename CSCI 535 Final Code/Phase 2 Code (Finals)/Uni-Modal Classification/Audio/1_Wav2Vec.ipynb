{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fBZrEqxHTJUd"},"outputs":[],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GqqDwVtpTn_E"},"outputs":[],"source":["#!pip install torchaudio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"33nktJ_Znwen"},"outputs":[],"source":["!pip install hydra-core omegaconf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NV3J4j93oIRd"},"outputs":[],"source":["!pip install bitarray"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VmW1G-l3om8b"},"outputs":[],"source":["!pip uninstall fairseq\n","!pip install fairseq"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mXjQerNkrVdP"},"outputs":[],"source":["!pip install fairseq\n","import os\n","# os.makedirs('/content/drive/MyDrive/535/WAV2VEC2', exist_ok=True)\n","\n","%cd /content/drive/MyDrive/535/WAV2VEC2\n","\n","!fairseq-hub install 'facebook/wav2vec2-large-960h-lv60-self'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mghKp6tDtDZf"},"outputs":[],"source":["!mkdir -p /content/drive/MyDrive/535/WAV2VEC2_XLSR53\n","!wget https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec2-large-xlsr-53.zip -P /content/drive/MyDrive/535/WAV2VEC2_XLSR53\n","!unzip /content/drive/MyDrive/535/WAV2VEC2_XLSR53/wav2vec2-large-xlsr-53.zip -d /content/drive/MyDrive/535/WAV2VEC2_XLSR53\n"]},{"cell_type":"markdown","source":["-----------------------------------------------------"],"metadata":{"id":"a9piiXOpy4K4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TU6z2_7BngW-"},"outputs":[],"source":["import os\n","import numpy as np\n","import soundfile as sf\n","import torch\n","from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n","import json\n","\n","# Load the pre-trained wave2vec model\n","model_name = 'facebook/wav2vec2-large-960h-lv60-self'\n","tokenizer = Wav2Vec2Tokenizer.from_pretrained(model_name)\n","model = Wav2Vec2ForCTC.from_pretrained(model_name)\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","# Load the metadata\n","metadata_path = '/content/drive/MyDrive/535/IEMOCAP_full_release/meta_data/meta_data.json'\n","\n","with open(metadata_path, 'r') as f:\n","    metadata = json.load(f)\n","\n","# Define a list to store the features and labels\n","features = []\n","labels = []\n","\n","audio_dir = '/content/drive/MyDrive/535/IEMOCAP_full_release/'\n","counter = 0\n","# Loop through each item in the metadata file\n","for item in metadata['meta_data']:\n","    if counter == 3000:\n","        break\n","    counter+=1\n","    print(counter)\n","    # Load the audio file and extract the relevant segment\n","    audio_file_path = os.path.join(audio_dir, item['path'])\n","    emotion_label = item['label']\n","\n","    input_audio, _ = sf.read(audio_file_path, dtype='float32')\n","\n","    # Convert the audio segment to a PyTorch tensor of input values\n","    input_values = tokenizer(input_audio, return_tensors='pt').input_values\n","\n","    # Pass the input values through the Wav2Vec2 model to obtain the features\n","    with torch.no_grad():\n","        output = model(input_values)\n","        features_tensor = output.logits.squeeze().cpu().numpy()\n","\n","    # Store the features and labels in the lists\n","    features.append(features_tensor)\n","    labels.append(metadata['labels'][emotion_label])\n","    \n","# Convert the lists to numpy arrays\n","features = np.array(features)\n","labels = np.array(labels)\n"]},{"cell_type":"code","source":["import pandas as pd\n","# Extracted feature vectors are stacked into a single numpy array called `features`.\n","\n","features_df = pd.DataFrame(features,columns=['features'])\n","labels_df = pd.DataFrame(labels, columns=['label'])\n","\n","# Concatenate the features, labels, and file names along the columns.\n","all_data_df = pd.concat([features_df, labels_df ], axis=1)\n","\n","# Save the features as a CSV file.\n","all_data_df.to_csv('/content/drive/MyDrive/535/WAV2VEC2/feaures.csv', index=False)"],"metadata":{"id":"2haa2qwHtNzv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","# Flatten feature vectors using numpy.ravel() method.\n","features_flat = np.array([f.ravel() for f in features])\n","\n","# Convert flattened feature vectors to a pandas DataFrame.\n","features_df = pd.DataFrame(features_flat, columns=['features'])\n","\n","# Create a DataFrame for labels.\n","labels_df = pd.DataFrame(labels, columns=['label'])\n","\n","# Concatenate the features, labels, and file names along the columns.\n","all_data_df = pd.concat([features_df, labels_df], axis=1)\n","\n","# Save the features as a CSV file.\n","all_data_df.to_csv('/content/drive/MyDrive/535/WAV2VEC2/features1.csv', index=False)\n"],"metadata":{"id":"ZMnP-JNRaJVk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import os\n","\n","# Flatten feature vectors using numpy.ravel() method.\n","features_flat = np.array([f.ravel() for f in features])\n","\n","# Split the features into chunks of maximum 10,000 records.\n","chunk_size = 10000\n","num_chunks = (len(features_flat) - 1) // chunk_size + 1\n","feature_chunks = np.array_split(features_flat, num_chunks)\n","\n","# Create a DataFrame for labels.\n","labels_df = pd.DataFrame(labels, columns=['label'])\n","\n","# Write each feature chunk to a separate file.\n","for i, chunk in enumerate(feature_chunks):\n","    chunk_df = pd.DataFrame(chunk, columns=['features'])\n","    all_data_df = pd.concat([chunk_df, labels_df], axis=1)\n","    file_name = f'/content/drive/MyDrive/535/WAV2VEC2/features_{i}.csv'\n","    all_data_df.to_csv(file_name, index=False)\n","\n","# Merge all the feature files into a single file.\n","file_names = [f'/content/drive/MyDrive/535/WAV2VEC2/features_{i}.csv' for i in range(num_chunks)]\n","all_data_df = pd.concat([pd.read_csv(f) for f in file_names], ignore_index=True)\n","\n","# Save the features as a CSV file.\n","all_data_df.to_csv('/content/drive/MyDrive/535/WAV2VEC2/features_formatted.csv', index=False)\n","\n","# Remove the temporary files.\n","for file_name in file_names:\n","    os.remove(file_name)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"frsWALZrnmpk","executionInfo":{"status":"ok","timestamp":1681692270390,"user_tz":420,"elapsed":1047,"user":{"displayName":"Shrutika Singh","userId":"15254678382012429631"}},"outputId":"2971d81a-4c09-445a-abd1-d4b812c7a535"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-111-02219c5c121c>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  features_flat = np.array([f.ravel() for f in features])\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import classification_report\n","\n","\n","# Load the features and labels from the CSV file\n","all_data_df = pd.read_csv('/content/drive/MyDrive/535/WAV2VEC2/features_formatted.csv')\n","\n","all_data_df['label'][0] = 0\n","\n","# Remove any rows with missing values\n","all_data_df = all_data_df.dropna()\n","\n","# Convert the features column from string to float\n","all_data_df['features'] = all_data_df['features'].apply(lambda x: np.fromstring(x[1:-1], sep=' '))\n","all_data_df = all_data_df[all_data_df['features'].apply(lambda x: x.shape == (3,))]\n","# for i, row in all_data_df.iterrows():\n","#     print(f\"Shape of feature array {i}: {row['features'].shape}\")\n","    \n","\n","# Split the data into training and testing sets\n","X = np.stack(all_data_df['features'].values)\n","y = all_data_df['label'].values.astype(int) # cast the labels to integer type\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train an SVC model\n","svc_model = SVC(kernel='linear', C=1, gamma='scale')\n","svc_model.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred = svc_model.predict(X_test)\n","\n","# Print the classification report.\n","print(classification_report(y_test, y_pred, zero_division=1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P0d6hRu3wnai","executionInfo":{"status":"ok","timestamp":1682033398202,"user_tz":420,"elapsed":393,"user":{"displayName":"Nishant Jethwa","userId":"17363613980376377701"}},"outputId":"89135955-4967-4154-d751-b77db06df2be"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\t        precision \t recall \tf1-score \tsupport\n","\t    0 \t     0.69\t   0.87 \t    0.79 \t     20\n","\t    1 \t     0.75\t   0.65 \t    0.71 \t     23\n","\t    2 \t     0.43\t   0.42 \t    0.44 \t     11\n","\t    3 \t     0.56\t   0.61 \t    0.59 \t     18\n","\t    4 \t     0.55\t   0.61 \t    0.59 \t     18\n","\t    5 \t     0.56\t   0.61 \t    0.59 \t     18\n","     accuracy  \t         \t       \t    \t    0.695\t     54\n","    macro avg \t     0.58\t   0.60 \t    0.60 \t     34\n"," weighted avg \t     0.64\t   0.66 \t    0.66 \t     45\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}